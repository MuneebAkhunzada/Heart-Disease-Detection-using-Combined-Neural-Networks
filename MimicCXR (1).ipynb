{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ru-2euIbkEkQ",
        "outputId": "acbde1d1-801f-4e8f-cfb7-99c47c00743f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.6.17)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.2.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload the kaggle.json file\n",
        "from google.colab import files\n",
        "files.upload()  # Upload kaggle.json file again if needed\n",
        "# Remove the conflicting .kaggle file (if it exists)\n",
        "!rm -f /root/.kaggle\n",
        "\n",
        "# Create the .kaggle directory\n",
        "!mkdir -p /root/.kaggle\n",
        "# Skip mkdir and directly copy kaggle.json\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "\n",
        "# Set proper permissions for the file\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "Eu38vktekDMI",
        "outputId": "c43d42f7-bbe6-4992-f257-240630a4044b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-cb488f2d-ac59-49da-9eba-3c3bfe43d7b8\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-cb488f2d-ac59-49da-9eba-3c3bfe43d7b8\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat 'kaggle.json': No such file or directory\n",
            "chmod: cannot access '/root/.kaggle/kaggle.json': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m6FWHWtApFEj",
        "outputId": "3d16dd5d-8409-48f0-a8a2-3b3171768513"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/kaggle\", line 5, in <module>\n",
            "    from kaggle.cli import main\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/kaggle/__init__.py\", line 7, in <module>\n",
            "    api.authenticate()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/kaggle/api/kaggle_api_extended.py\", line 407, in authenticate\n",
            "    raise IOError('Could not find {}. Make sure it\\'s located in'\n",
            "OSError: Could not find kaggle.json. Make sure it's located in /root/.config/kaggle. Or use the environment method. See setup instructions at https://github.com/Kaggle/kaggle-api/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d itsanmol124/mimic-cxr"
      ],
      "metadata": {
        "id": "gTJ8GdLefi4m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a506b1aa-eb33-43bc-b35d-a226039dcad0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/itsanmol124/mimic-cxr\n",
            "License(s): unknown\n",
            "Downloading mimic-cxr.zip to /content\n",
            "100% 2.04G/2.04G [00:31<00:00, 32.2MB/s]\n",
            "100% 2.04G/2.04G [00:31<00:00, 68.9MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "# Unzip the dataset\n",
        "with zipfile.ZipFile(\"/content/mimic-cxr.zip\", \"r\") as zip_ref:\n",
        "    zip_ref.extractall(\"/content/mimic_cxr\")\n",
        "print(\"Dataset extracted to /content/mimic_cxr\")"
      ],
      "metadata": {
        "id": "R9R-G71YmZap",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5337d5f8-4401-4f83-eb21-021614d2e107"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset extracted to /content/mimic_cxr\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "def create_hybrid_model(input_shape, num_labels):\n",
        "    inputs = tf.keras.Input(shape=input_shape)\n",
        "    x = layers.Conv2D(32, (3, 3), activation=\"relu\")(inputs)\n",
        "    x = layers.MaxPooling2D((2, 2))(x)\n",
        "    x = layers.Conv2D(64, (3, 3), activation=\"relu\")(x)\n",
        "    x = layers.MaxPooling2D((2, 2))(x)\n",
        "    x = layers.Conv2D(128, (3, 3), activation=\"relu\")(x)\n",
        "    x = layers.MaxPooling2D((2, 2))(x)\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dense(256, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    x = layers.Dense(128, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    outputs = layers.Dense(num_labels, activation=\"sigmoid\")(x)\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model"
      ],
      "metadata": {
        "id": "dUMKQF4WuVrt"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load CSV file\n",
        "extract_dir = \"/content/mimic_cxr\"\n",
        "csv_file = os.path.join(extract_dir, \"mimic-cxr.csv\")\n",
        "\n",
        "# Step 2: Load CSV and define features\n",
        "data_df = pd.read_csv(csv_file)\n",
        "tabular_features = [\n",
        "    \"Atelectasis\", \"Cardiomegaly\", \"Consolidation\", \"Edema\",\n",
        "    \"Enlarged Cardiomediastinum\", \"Lung Lesion\", \"Lung Opacity\",\n",
        "    \"Pleural Effusion\", \"Pneumonia\", \"Pneumothorax\"\n",
        "]\n",
        "for feature in tabular_features:\n",
        "    data_df[feature] = data_df[feature].astype(np.float32)\n",
        "\n",
        "# Preprocess the label column for multi-label classification\n",
        "all_labels = [\n",
        "    \"Normal\", \"Atelectasis\", \"Cardiomegaly\", \"Consolidation\", \"Edema\",\n",
        "    \"Enlarged Cardiomediastinum\", \"Lung Lesion\", \"Lung Opacity\",\n",
        "    \"Pleural Effusion\", \"Pneumonia\", \"Pneumothorax\"\n",
        "]\n",
        "\n",
        "def process_labels(label_str):\n",
        "    \"\"\"Convert label strings to one-hot encoded vectors.\"\"\"\n",
        "    labels = label_str.split(\", \")\n",
        "    return [1 if label in labels else 0 for label in all_labels]\n",
        "\n",
        "data_df['label'] = data_df['label'].apply(process_labels)\n",
        "\n",
        "\n",
        "data_df[tabular_features] = data_df[tabular_features].astype(float)\n",
        "\n",
        "#data_df['filepath'] = data_df['filename'].apply(lambda x: os.path.join(extract_dir, x))\n",
        "data_df['filepath'] = data_df.apply(lambda row: os.path.join(extract_dir, row['split'], row['filename']), axis=1)\n"
      ],
      "metadata": {
        "id": "VX0eJ_OhAsfT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Prepare undersampled training dataset\n",
        "train_data = data_df[data_df['split'] == 'train']\n",
        "valid_data = data_df[data_df['split'] == 'valid']\n",
        "test_data = data_df[data_df['split'] == 'test']\n",
        "\n",
        "print(f\"Training Set: {len(train_data)}\")\n",
        "print(f\"Validation Set: {len(valid_data)}\")\n",
        "print(f\"Testing Set: {len(test_data)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOmCG1ymWVBs",
        "outputId": "e69c94b9-9626-4813-a793-5c9efef56f59"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Set: 83837\n",
            "Validation Set: 711\n",
            "Testing Set: 1455\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Undersample training dataset\n",
        "undersampled_train_data = train_data.sample(n=10000, random_state=42)\n",
        "\n",
        "# Combine valid and test datasets without modification\n",
        "valid_data.reset_index(drop=True, inplace=True)\n",
        "test_data.reset_index(drop=True, inplace=True)\n",
        "\n",
        "print(f\"Undersampled Training Set: {len(undersampled_train_data)}\")\n",
        "print(f\"Validation Set: {len(valid_data)}\")\n",
        "print(f\"Testing Set: {len(test_data)}\")\n",
        "\n",
        "# Verify label column is one-hot encoded\n",
        "print(undersampled_train_data['label'].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubXUTEA-WWqr",
        "outputId": "fa5e68f0-b4a6-4f69-b1cd-fba49da7e25d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Undersampled Training Set: 10000\n",
            "Validation Set: 711\n",
            "Testing Set: 1455\n",
            "43437    [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "42418    [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "44136    [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0]\n",
            "65209    [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
            "14608    [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Name: label, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Define preprocessing functions\n",
        "# Preprocessing functions\n",
        "def preprocess_image(image_path):\n",
        "    image = tf.io.read_file(image_path)\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    image = tf.image.resize(image, [224, 224])\n",
        "    image /= 255.0  # Normalize to [0, 1]\n",
        "    return image\n",
        "\n",
        "def preprocess_tabular_data(tabular_values):\n",
        "    # Explicitly cast tabular values to float32\n",
        "    tabular_tensor = tf.convert_to_tensor(tabular_values, dtype=tf.float32)\n",
        "    return tabular_tensor\n",
        "\n",
        "def load_and_preprocess(filepath, label, tabular_values):\n",
        "    \"\"\"Load and preprocess image, tabular data, and label.\"\"\"\n",
        "    image = preprocess_image(filepath)\n",
        "    tabular_data = preprocess_tabular_data(tabular_values)\n",
        "\n",
        "    # Cast label to float32\n",
        "    label = tf.cast(label, dtype=tf.float32)\n",
        "\n",
        "    return (image, tabular_data), label"
      ],
      "metadata": {
        "id": "YpYA47nvWW-H"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_tf_dataset(data):\n",
        "    filepaths = data[\"filepath\"].tolist()\n",
        "    labels = np.array(data[\"label\"].tolist())  # Convert labels to NumPy arrays\n",
        "    tabular_data = data[tabular_features].values.astype(np.float32)  # Ensure tabular data is float32\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((filepaths, labels, tabular_data))\n",
        "    dataset = dataset.map(\n",
        "        lambda filepath, label, tabular_values: load_and_preprocess(filepath, label, tabular_values),\n",
        "        num_parallel_calls=tf.data.AUTOTUNE\n",
        "    )\n",
        "    return dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "train_dataset = create_tf_dataset(undersampled_train_data)\n",
        "valid_dataset = create_tf_dataset(valid_data)\n",
        "test_dataset = create_tf_dataset(test_data)"
      ],
      "metadata": {
        "id": "9BDLeqC_WXmn"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hybrid Model Definition\n",
        "image_input = layers.Input(shape=(224, 224, 3), name=\"image_input\")\n",
        "image_model = tf.keras.applications.ResNet50(weights=\"imagenet\", include_top=False, input_tensor=image_input)\n",
        "image_features = layers.Flatten()(image_model.output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFkEwmeoWn-M",
        "outputId": "25282efa-e832-4620-bf14-ceb6fc6d7013"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tabular input\n",
        "tabular_input = layers.Input(shape=(len(tabular_features),), name=\"tabular_input\")\n",
        "tabular_features_dense = layers.Dense(128, activation=\"relu\")(tabular_input)\n",
        "\n",
        "# Combine image and tabular features\n",
        "combined = layers.concatenate([image_features, tabular_features_dense])\n",
        "combined_dense = layers.Dense(128, activation=\"relu\")(combined)\n",
        "output = layers.Dense(11, activation=\"sigmoid\")(combined_dense)\n",
        "\n",
        "# Build the hybrid model\n",
        "hybrid_model = models.Model(inputs=[image_input, tabular_input], outputs=output)\n",
        "hybrid_model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Train the model\n",
        "epochs = 30\n",
        "history = hybrid_model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=valid_dataset,\n",
        "    epochs=epochs\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGz-YZq2WoGY",
        "outputId": "da36ca47-c910-4715-b921-bd0bd37c65c2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 405ms/step - accuracy: 0.4102 - loss: 0.7329 - val_accuracy: 0.7932 - val_loss: 0.2761\n",
            "Epoch 2/30\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 297ms/step - accuracy: 0.7930 - loss: 0.0505 - val_accuracy: 0.8650 - val_loss: 0.0349\n",
            "Epoch 3/30\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 301ms/step - accuracy: 0.8176 - loss: 0.0086 - val_accuracy: 0.8594 - val_loss: 0.0047\n",
            "Epoch 4/30\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 302ms/step - accuracy: 0.8218 - loss: 0.0029 - val_accuracy: 0.8608 - val_loss: 0.0017\n",
            "Epoch 5/30\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 303ms/step - accuracy: 0.8267 - loss: 0.0013 - val_accuracy: 0.8523 - val_loss: 0.0010\n",
            "Epoch 6/30\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 302ms/step - accuracy: 0.8284 - loss: 6.7294e-04 - val_accuracy: 0.8495 - val_loss: 6.9319e-04\n",
            "Epoch 7/30\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 303ms/step - accuracy: 0.8221 - loss: 4.0300e-04 - val_accuracy: 0.8495 - val_loss: 5.5150e-04\n",
            "Epoch 8/30\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 302ms/step - accuracy: 0.8219 - loss: 2.5782e-04 - val_accuracy: 0.8411 - val_loss: 5.1848e-04\n",
            "Epoch 9/30\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 300ms/step - accuracy: 0.8203 - loss: 1.7328e-04 - val_accuracy: 0.8368 - val_loss: 5.0471e-04\n",
            "Epoch 10/30\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 302ms/step - accuracy: 0.8165 - loss: 1.1911e-04 - val_accuracy: 0.8256 - val_loss: 5.1233e-04\n",
            "Epoch 11/30\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 303ms/step - accuracy: 0.8115 - loss: 8.1636e-05 - val_accuracy: 0.8200 - val_loss: 6.5978e-04\n",
            "Epoch 12/30\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 301ms/step - accuracy: 0.8070 - loss: 5.6242e-05 - val_accuracy: 0.8256 - val_loss: 7.8721e-04\n",
            "Epoch 13/30\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 299ms/step - accuracy: 0.8069 - loss: 3.9562e-05 - val_accuracy: 0.8143 - val_loss: 0.0012\n",
            "Epoch 14/30\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 300ms/step - accuracy: 0.8037 - loss: 2.8245e-05 - val_accuracy: 0.8242 - val_loss: 0.0025\n",
            "Epoch 15/30\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 300ms/step - accuracy: 0.8057 - loss: 2.0203e-05 - val_accuracy: 0.8256 - val_loss: 0.0043\n",
            "Epoch 16/30\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 299ms/step - accuracy: 0.8046 - loss: 1.4952e-05 - val_accuracy: 0.8242 - val_loss: 0.0063\n",
            "Epoch 17/30\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 299ms/step - accuracy: 0.8038 - loss: 1.1435e-05 - val_accuracy: 0.8256 - val_loss: 0.0075\n",
            "Epoch 18/30\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 302ms/step - accuracy: 0.7983 - loss: 3.1630e-04 - val_accuracy: 0.1378 - val_loss: 1704180.5000\n",
            "Epoch 19/30\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 296ms/step - accuracy: 0.7971 - loss: 0.0015 - val_accuracy: 0.7750 - val_loss: 1.3282e-04\n",
            "Epoch 20/30\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 295ms/step - accuracy: 0.7643 - loss: 6.1454e-05 - val_accuracy: 0.7117 - val_loss: 0.0066\n",
            "Epoch 21/30\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 297ms/step - accuracy: 0.7925 - loss: 3.6505e-05 - val_accuracy: 0.7778 - val_loss: 2.1459e-05\n",
            "Epoch 22/30\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 298ms/step - accuracy: 0.7831 - loss: 1.7028e-05 - val_accuracy: 0.7764 - val_loss: 1.4764e-05\n",
            "Epoch 23/30\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 296ms/step - accuracy: 0.7868 - loss: 1.3988e-05 - val_accuracy: 0.7679 - val_loss: 1.2470e-05\n",
            "Epoch 24/30\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 296ms/step - accuracy: 0.7835 - loss: 1.2034e-05 - val_accuracy: 0.7567 - val_loss: 1.0825e-05\n",
            "Epoch 25/30\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 299ms/step - accuracy: 0.7812 - loss: 1.0548e-05 - val_accuracy: 0.7623 - val_loss: 9.5498e-06\n",
            "Epoch 26/30\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 297ms/step - accuracy: 0.7810 - loss: 9.3360e-06 - val_accuracy: 0.7665 - val_loss: 8.4230e-06\n",
            "Epoch 27/30\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 297ms/step - accuracy: 0.7803 - loss: 8.2583e-06 - val_accuracy: 0.7693 - val_loss: 7.4989e-06\n",
            "Epoch 28/30\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 297ms/step - accuracy: 0.7826 - loss: 7.3882e-06 - val_accuracy: 0.7722 - val_loss: 6.6621e-06\n",
            "Epoch 29/30\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 299ms/step - accuracy: 0.7839 - loss: 6.5533e-06 - val_accuracy: 0.7736 - val_loss: 5.9103e-06\n",
            "Epoch 30/30\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 297ms/step - accuracy: 0.7841 - loss: 5.8534e-06 - val_accuracy: 0.7722 - val_loss: 5.2683e-06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_5h-AWV0ExC0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}